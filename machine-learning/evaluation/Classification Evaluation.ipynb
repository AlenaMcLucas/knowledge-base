{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported by sklearn: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Labels\n",
    "\n",
    "Binary classification\n",
    "\n",
    "- precision_recall_curve(y_true, probas_pred)\n",
    "- roc_curve(y_true, y_score)\n",
    "\n",
    "Binary + multiclass classification\n",
    "\n",
    "- balanced_accuracy_score(y_true, y_pred)\n",
    "- cohen_kappa_score(y1, y2)\n",
    "- confusion_matrix(y_true, y_pred)\n",
    "- hinge_loss(y_true, pred_decision)\n",
    "- matthews_corrcoef(y_true, y_pred)\n",
    "- roc_auc_score(y_true, y_score)\n",
    "\n",
    "Binary + multiclass + multilabel classification\n",
    "\n",
    "- accuracy_score(y_true, y_pred)\n",
    "- classification_report(y_true, y_pred)\n",
    "- f1_score(y_true, y_pred)\n",
    "- fbeta_score(y_true, y_pred)\n",
    "- hamming_loss(y_true, y_pred)\n",
    "- jaccard_score(y_true, y_pred)\n",
    "- log_loss(y_true, y_pred)\n",
    "- multilabel_confusion_matrix(y_true, y_pred)\n",
    "- precision_recall_fscore_support(y_true)\n",
    "- precision_score(y_true, y_pred)\n",
    "- recall_score(y_true, y_pred)\n",
    "- roc_auc_score(y_true, y_score)\n",
    "- zero_one_loss(y_true, y_pred)\n",
    "\n",
    "Binary + multilabel classification\n",
    "\n",
    "- average_precision_score(y_true, y_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "![](img/tptnfpfn.png)\n",
    "\n",
    "# Confusion Matrix - Extended\n",
    "\n",
    "![](img/tptnfpfn-extended.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support\n",
    "\n",
    "Support is the number of actual occurrences of the class in the test data set. Imbalanced support in the training data may indicate the need for stratified sampling or rebalancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision\n",
    "\n",
    "What proportion of predictied positives is truly positive? In other words, of all the actual positive class, how many were correctly predicted? In the case of multi-class classification, this is calculated for each class (this class / not this class):\n",
    "\n",
    "$\\frac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall\n",
    "\n",
    "aka Sensitivity\n",
    "\n",
    "What proportion of actual positives are predicted positive? In other words, of all the predicted positive class, how many were actually positive? In the case of multi-class classification, this is calculated for each class (this class / not this class):\n",
    "\n",
    "$\\frac{TP}{TP + FN}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specificity\n",
    "\n",
    "Number of examples correctly predicted to be negative out of total true negatives\n",
    "\n",
    "$\\frac{TN}{TN + FP}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type I Error\n",
    "\n",
    "aka False Positive Rate\n",
    "\n",
    "Number of examples falsely predicted to be positive out of total true negatives\n",
    "\n",
    "$\\frac{FP}{FP + TN}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type II Error\n",
    "\n",
    "aka False Negative Rate\n",
    "\n",
    "Number of examples falsely predicted to be negative out of total true positives\n",
    "\n",
    "$\\frac{FN}{FN + TP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "\n",
    "What proportion of all classes were correctly predicted?\n",
    "\n",
    "$\\frac{TP + TN}{TP + TN + FP + FN}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced Accuracy\n",
    "\n",
    "Binary classification, balanced accuracy:\n",
    "\n",
    "$\\frac{1}{2} ( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP} )$\n",
    "\n",
    "To extend to multiclass classification,\n",
    "\n",
    "$\\frac{1}{\\text{n classes}} \\sum \\frac{TP}{TP + FN}$\n",
    "\n",
    "over each class,  where $\\frac{\\text{correctly classified}}{\\text{total true in class}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Per-Class) F1-Score\n",
    "\n",
    "F1-score uses the harmonic mean instead of the arithmetic mean:\n",
    "\n",
    "$2 \\times \\frac{precision \\times recall}{precision + recall}$\n",
    "\n",
    "It will always be between precision and recall, but it gives a larger weight to / penalizes lower numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Combined) Macro-F1, Macro-Precision, Macro-Recall\n",
    "\n",
    "Arithmetic mean of per-class f1-scores, precision, and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Combined) Weighted-F1, Weighted-Precision, Weighted-Recall\n",
    "\n",
    "Weighted average of per-class f1-scores, precision, and recall by the number of examples in each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Combined) Micro-F1, Micro-Precision, Micro-Recall\n",
    "\n",
    "Calculate micro-precision and micro-recall by calculating each metric as a sum across all classes. TP is a sum across all classes and will have some value. Because FP and FN look for false predictions across all classes, FP == FN for all classes instead of for each class individually. This causes precision == recall, which causes f1-score to be the same. Calculating accuracy from the generalize formula above, we can also prove that accuracy is the same. So:\n",
    "\n",
    "precision = recall = f1-score = accuracy\n",
    "\n",
    "Great article that covers this: https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1-Score Caution\n",
    "\n",
    "As the eminent statistician David Hand explained, “the relative importance assigned to precision and recall should be an aspect of the problem”. Classifying a sick person as healthy has a different cost from classifying a healthy person as sick, and this should be reflected in the way weights and costs are used to select the best classifier for the specific problem you are trying to solve. The standard F1-scores do not take any of the domain knowledge into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathews Correlation Coefficient (MCC)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Matthews_correlation_coefficient\n",
    "\n",
    "MCC, originally devised for binary classification on unbalanced classes, has been extended to evaluates multiclass classifiers by computing the correlation coefficient between the observed and predicted classifications. A coefficient of +1 represents a perfect prediction, 0 is similar to a random prediction and −1 indicates an inverse prediction.\n",
    "\n",
    "$\\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$\n",
    "\n",
    "MCC is more informative than F1 score and accuracy in evaluating binary classifications problems because it takes into account the balance  ratios / size of the four confusion matrix categories (TP, TN, FP, FN). This is especially helpful when dealing with a highly imbalanced dataset, since your algorithm may be predicting most/all of the dominant class. By considering the proportion of each class of the confusion matrix in its formula, its score is high only if your classifer is doing well on both the negative and positive examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kappa Score, aka Cohen's Kappa Coefficient\n",
    "\n",
    "This score measures the degree of agreement between two evaluators (admissions example), but in data science we are looking at the agreement between the true and predicted values. The kappa score considers how much better the agreements are over and beyond chance agreements. Thus, in addition to Agree, the kappa formula also uses the expected proportion of chance agreements; let’s call this number ChanceAgree.\n",
    "\n",
    "$KappaScore = \\frac{Agree-ChanceAgree}{1-ChanceAgree} $\n",
    "\n",
    "Note that the numerator calculates the difference between Agree and ChanceAgree. If Agree=1, we have perfect agreement. In this case, the kappa score is 1, regardless of ChanceAgree. In contrast, if Agree=ChanceAgree, kappa is 0, signifying that the professors’ agreement is by chance. If Agree is smaller than ChanceAgree, the kappa score is negative, denoting that the degree of agreement is lower than chance agreement.\n",
    "\n",
    "$Agree = \\frac{TP}{N}$ across all examples\n",
    "\n",
    "To calculate the probability of a particular class for each evaluator, calculate the following for each class:\n",
    "\n",
    "$Prob_{true}(ThisClass) = \\frac{N_{ThisClass}}{N}, $\n",
    "$Prob_{pred}(ThisClass) = \\frac{N_{ThisClass}}{N}$\n",
    "\n",
    "To calculate ChanceAgree for a particular class:\n",
    "\n",
    "$ChanceAgree(ThisClass) = Prob_{true}(ThisClass) \\times Prob_{pred}(ThisClass)$\n",
    "\n",
    "Summing up the above probability for each class, we get the probability that agreement on any of the classes happened by chance:\n",
    "\n",
    "$ChanceAgree = ChanceAgree(ThisClass)+ChanceAgree(AnotherClass)+ChanceAgree(YetAnotherClass)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the best metrics for evaluating multi-class classifiers on imbalanced datasets.\n",
    "\n",
    "The traditional metrics from the classification report are biased towards the majority class and assumes an identical distribution of the actual and predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy / Log Loss\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "Log loss is usually used when there are just two possible outcomes that can be either 0 or 1. Cross entropy is usually used when there are three or more possible outcomes.\n",
    "\n",
    "Cross-entropy measures the extent to which the predicted probabilities match the given data, and is useful for probabilistic classifiers such as Naïve Bayes. It is a more generic form of the logarithmic loss function, which was derived from neural network architecture, and is used to quantify the cost of inaccurate predictions. The classifier with the lowest log loss is preferred.\n",
    "\n",
    "(Most associated with Log Loss)\n",
    "\n",
    "$- \\sum^N_i \\sum^M_j y_{ij} \\times ln p_{ij}$\n",
    "\n",
    "\n",
    "Example on how to calculate:\n",
    "\n",
    "1. Make a one-hot encoding of the true labels, eg. $y_1$=A=[1,0,0]\n",
    "2. Use your model to predict probabilities , eg. $p_1$=[0.7,0.2,0.1]\n",
    "3. Take the element-wise product of the label and probability: $y_1⋅p_1=ln(0.7)$\n",
    "4. Do the same for all the other samples and take the negative of the sum\n",
    "\n",
    "https://jamesmccaffrey.wordpress.com/2016/09/25/log-loss-and-cross-entropy-are-almost-the-same/#:~:text=Log%20loss%20is%20usually%20used,three%20or%20more%20possible%20outcomes.&text=In%20words%2C%20cross%20entropy%20is,probabilities%20times%20the%20actual%20probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation I've had more success with (more associated with Cross-Entropy):\n",
    "\n",
    "$-(y_t log(y_p) + (1 - y_t) log(1 - y_p))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22839300363692283"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logloss(y_true, y_pred):\n",
    "    return -(1 / len(y_true)) * sum([( t * log(p) ) + ( ( 1 - t ) * ( log(1 - p) ) ) for t,p in zip(y_true, y_pred)]) \n",
    "\n",
    "logloss([1,0,0], [0.7,0.2,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22839300363692283"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss([1,0,0], [0.7,0.2,0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC-AUC Score\n",
    "\n",
    "AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.\n",
    "\n",
    "AUC helps us to compare one ROC curve to another by calculating the area under the curve. Generally, the more area it captures the better.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Some important characteristics of ROC-AUC are:\n",
    "\n",
    "- The value can range from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0. However auc score of a random classifier for balanced data is 0.5\n",
    "- ROC-AUC score is independent of the threshold set for classification because it only considers the rank of each prediction and not its absolute value. The same is not true for F1 score which needs a threshold value in case of probabilities output\n",
    "\n",
    "AUC is desirable for the following two reasons:\n",
    "\n",
    "- AUC is **scale-invariant**. It measures how well predictions are ranked, rather than their absolute values.\n",
    "- AUC is **classification-threshold-invariant**. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n",
    "\n",
    "However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:\n",
    "\n",
    "- **Scale invariance is not always desirable.** For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.\n",
    "- **Classification-threshold invariance is not always desirable.** In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.\n",
    "\n",
    "---\n",
    "\n",
    "The probabilistic interpretation of ROC-AUC score is that if you randomly choose a positive case and a negative case, the probability that the positive case outranks the negative case according to the classifier is given by the AUC.\n",
    "\n",
    "Mathematically, it is calculated by area under curve of sensitivity (TPR) vs.\n",
    "FPR(1-specificity). Ideally, we would like to have high sensitivity & high specificity, but in real-world scenarios, there is always a tradeoff between sensitivity & specificity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-Recall Curve\n",
    "\n",
    "The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision. This is more commonly used with binary classification but can be used in multi-label classification as well.\n",
    "\n",
    "![](img/precision-recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve\n",
    "\n",
    "Start predicting every example as positive and calculate a point. Then increase the threshold until one sample would be predicted as negative. Then two. And so on until all examples are predicted to be negative. Plot each of those points.\n",
    "\n",
    "Plot a point at (True Positive Rate, False Positive Rate).\n",
    "\n",
    "Another way to interpret a point (0, 0.75) is that the model is correctly classifying 75% of positive samples and 100% of negative samples.\n",
    "\n",
    "Depending on how many false positives or false negatives you're willing to accept in application, an optimal point can be selected.\n",
    "\n",
    "You can replace False Positives with Precision when you have a highly imbalanced dataset where there are mostly negative examples.\n",
    "\n",
    "---\n",
    "\n",
    "The point that is farthest to the left of the slope = 1 line is the point at which the threshold decreases the proportion of the examples that are incorrectly classified as positive (false positives).\n",
    "\n",
    "![](img/roc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
