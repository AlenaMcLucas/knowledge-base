{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95360e30",
   "metadata": {},
   "source": [
    "# Regression Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68513a07",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE)\n",
    "\n",
    "- MAE calculates the absolute different between actual and predicted values\n",
    "- $MAE= \\frac{1}{n} \\sum | y-\\hat{y} | $\n",
    "- Advantages:\n",
    "    - MAE is in the same units as the dependent variable\n",
    "    - Most robust to outliers, meaning a few large errors doesn't overpower a lot of smaller ones\n",
    "- Disadvantages:\n",
    "    - The graph of MAE is not differentiable so it's not a great loss function; we have to apply variables optimizers like gradient descent which can be differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741cb58",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)\n",
    "\n",
    "- MSE calculates the squared difference between actual and predicted values\n",
    "- As a loss function, it's commonly referred to as \"least squares\"\n",
    "- $MSE= \\frac{1}{n} \\sum (y-\\hat{y})^2 $\n",
    "- Advantages:\n",
    "    - The graph of MSE is differentiable, so you can easily use it as a loss function\n",
    "- Disadvantages:\n",
    "    - MSE is a squared unit of output, so interpretation can be harder\n",
    "    - It is not robust to outliers. It punishes the model more for large errors because they're squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3f4b6",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE)\n",
    "\n",
    "- RMSE calculates takes the square root of MSE\n",
    "- $RMSE= \\sqrt{\\frac{1}{n} \\sum (y-\\hat{y})^2}$\n",
    "- Advantages:\n",
    "    - RMSE is in the same units as the dependent variable\n",
    "- Disadvantages:\n",
    "    - It is not as robust to outliers as MAE, but much more robust than MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b005775",
   "metadata": {},
   "source": [
    "## R-Squared ($R^2$)\n",
    "\n",
    "- $R^2$ measures the proportion of variance the independent variables + model explain of the dependent variable. It is often used in model comparison.\n",
    "- It is also known as Coefficient of Determination or Goodness of Fit.\n",
    "- $R^2= 1 - \\frac{SSR}{SST}$\n",
    "- $SSR = \\sum (y - \\hat{y})^2$\n",
    "- $SST = \\sum (y - \\bar{y})^2$\n",
    "- The range is 0-1. Interpret an $R^2 = 0.8$ to mean that the model explains 80% of the variance in the data\n",
    "- Advantages:\n",
    "    - Compare different models\n",
    "- Disadvantages:\n",
    "    - It is not as robust to outliers as MAE.\n",
    "    - Adding new features makes $R^2$ either stay the same or increase, even if the features are irrelevant.\n",
    "    - Not interpretable in the same units as the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb5609",
   "metadata": {},
   "source": [
    "## Adjusted R-Squared (Adj $R^2$)\n",
    "\n",
    "- $R^2_a = 1 - [\\frac{(1 - R^2)(n - 1)}{n - k - 1}]$\n",
    "- Punishes the model when additional features are added.\n",
    "- Advantages:\n",
    "    - Compare different models\n",
    "    - Much less affected by adding irrelevant features\n",
    "- Disadvantages:\n",
    "    - Not interpretable in the same units as the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2472a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
