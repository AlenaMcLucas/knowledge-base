{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0124ccc",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Summary\n",
    "\n",
    "\n",
    "- **Regression**: a method for modeling the relationship between one dependent and one or more independent variables\n",
    "\n",
    "Linear regression describes the linear relationship between the independent variables and the dependent one. Linear regression solves for the \"line of best fit\".\n",
    "\n",
    "**Keywords**:\n",
    "- supervised learning\n",
    "- regression\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- **Linearity of residuals**: There needs to be a linear relationship between the dependent variable and independent variables\n",
    "- **Independence of residuals**: Error should not be correlated with the dependent variable.\n",
    "\n",
    "![](../img/independence-residuals.jpg)\n",
    "\n",
    "- **Normal distribution of residuals**: Error should be normally distributed, with a mean near 0.\n",
    "- **Equal variance of residuals**: Error must have constance variance, or homoscedasticity.\n",
    "\n",
    "![](../img/homoscedasticity.jpg)\n",
    "\n",
    "- **No perfect or very high multicollinearity**\n",
    "\n",
    "### Pros\n",
    "\n",
    "- E\n",
    "\n",
    "### Cons\n",
    "\n",
    "- As feature size increases, the model becomes more prone to overfitting\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "- C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae24be",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "To calculate the best-fit line, linear regression uses a traditional slose-intercept form: $y_i = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$\n",
    "\n",
    "Mathematically, the best fit line is obtained by minimizing the **Residual Sum of Squares (RSS)**.\n",
    "\n",
    "While others exist, generally linear regression uses **Mean Squared Error (MSE)** as its **cost / loss function**: $MSE=\\frac{1}{N}\\sum{(y_i - (\\beta_0+\\beta_1x_1+ ... + \\beta_nx_n))^2}$.\n",
    "\n",
    "Steps for **gradient descent**, which aims to optimize the cost function by minimizing its error through an iterative process:\n",
    "\n",
    "1. Set coefficients to zero and user defines the **learning rate**. The learning rate defines how large each step will be. The larger it is the faster it converges, but it could end up being so large you overshoot or fail to converge at a minima. The smaller it is, the longer it converges but more accurate its results will be.\n",
    "2. Calculate the partial derivative of MSE, one time for each coefficient with respect to that coefficient. ( $D_{\\beta0}, D_{\\beta1}, \\text{etc}$ ). Here, we are calculating the steepness of the slope (value of the slope tangent to the curve). These are called the **gradients**.\n",
    "3. Update the coefficients: $\\beta_0 = \\beta_0 - L \\times D_{\\beta0}$, $\\beta_1 = \\beta_1 - L \\times D_{\\beta1}$, etc\n",
    "    1. Where L is the learning rate.\n",
    "4. Repeat steps 2 and 3 until the cost function is a very small value, or ideally 0 (0 error means 100% prediction accuracy).\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "For **logistic regression**, which has a binary dependent variable, we use the equation $logit(p_i) = ln(\\frac{p_i}{1-p_i}) = \\beta_0 + \\beta_1x_{1,i} + ... + \\beta_nx_{n,i}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b714051",
   "metadata": {},
   "source": [
    "## Bias Variance Trade-Off\n",
    "\n",
    "- **Bias** measures how accurate the model is likely to be on future data.\n",
    "- Generally, linear algorithms have a high bias. They are likely to perform better on new data, but they are also simple and less flexible.\n",
    "- **Variance** measures the sensitivity of the model towards the training data, or how much the model reacts when input data is changed.\n",
    "- Ideally, variance is low and the model doesn't change much because the algorithm is well-suited to the underlying patterns in the training dataset. Another way to say this, is the model is more generalizable.\n",
    "- We want both low bias and low variance. However, the **bias variance trade-off** has an inverse relationship. An increase in one often leads to a decrease in the other.\n",
    "\n",
    "![](../img/bias-variance.jpg)\n",
    "\n",
    "## Overfitting and Underfitting\n",
    "\n",
    "- **Overfitting** occurs when a model learns every pattern and noice in the data to such an extent that it affects the performance of the model on unseen data. It interprets noice as patterns.\n",
    "    - This happens when a model has low bias and higher variance.\n",
    "    - To prevent overfitting, try:\n",
    "        - Cross-validation\n",
    "        - If the training data is too small, add more examples\n",
    "        - If the training data is too large, feature selection\n",
    "        - Regularization\n",
    "- **Underfitting** occurs when the model fails to learn from the training dataset and is also not able to generalize the test dataset. This usually leads to low training and test accuracy.\n",
    "    - This happens when a model has high bias and low variance.\n",
    "    - To prevent underfitting, try:\n",
    "        - Increasing the model complexity\n",
    "        - Increasing the number of features in the training data\n",
    "        - Removing noise from the data\n",
    "\n",
    "## Paradigms\n",
    "\n",
    "- **Statisticians paradigm** has the goal of understanding underlying causal relationships (inferential)\n",
    "    - General question: \"What is the causal effect of changes in an independent variable on changes in a dependent variable?\"\n",
    "    - Consistency and bias are important\n",
    "- **Predictive paradigm** has the goal of prediction and pattern recognition\n",
    "    - General question: \"How accurately can we predict a dependent variable based on independent variables?\"\n",
    "    - Goodness-of-fit takes precedence over bias and efficiency\n",
    "\n",
    "## Hypothesis Testing\n",
    "\n",
    "We are hypothesis testing, for each coefficient, if it explains the variance in the dependent variable.\n",
    "\n",
    "$H_0: B_1=0$\n",
    "\n",
    "$H_A: B_1\\ne0$\n",
    "\n",
    "To test, we use a **t-test**, a test statistic for each coefficient: $t=\\frac{m-\\mu}{s/\\sqrt{n}}$\n",
    "\n",
    "## Cautions\n",
    "\n",
    "- Regression finds correlations in data, it does not necessarily imply causation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c77ab",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "- R-Squared (R2), also known as the coefficient of determination, represents the amount of variation explained / captured by the model\n",
    "    - Ranges between 0-1\n",
    "    - The higher it is, the better the fit\n",
    "    - $R^2=1-\\frac{RSS}{TSS}$\n",
    "        - Where RSS is the Residual Sum of Squares: $RSS=\\sum(y-\\beta_0-\\beta_1x_1-...-\\beta_nx_n)^2$\n",
    "        - Where TSS is the Total Sum of Squares: $TSS=\\sum(y-\\bar{y})^2$\n",
    "            - Where $\\bar{y}$ is the mean of the data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed99b5",
   "metadata": {},
   "source": [
    "## Improving the Model\n",
    "\n",
    "- Look for **multicollinearity**, or relationships between independent variables. Remove redundancy when you identify it because it can make it difficult to determine which variable is contibuting towards the prediction of the dependent variable. Find multicollinearity by:\n",
    "    - Correlation\n",
    "    - **Variance Inflation Factor** (VIF) explains the relationship between two independent variables: $VIF=\\frac{1}{1-R^2}$\n",
    "        - If VIF > 10, then the value is high and it should be dropped. 5 < VIF < 10, then inspect, and VIF < 5 is a good value. \n",
    "- Spend time on feature selection, especially when there are many features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726349c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
