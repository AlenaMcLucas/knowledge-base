{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b163a1",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Keywords**:\n",
    "- supervised learning\n",
    "- classification\n",
    "    - binary\n",
    "    - multiclass\n",
    "- **lazy learner** - does not perform any training when you supply the training data\n",
    "- **non-parametric** - does not make any assumptions about the underlying data distribution\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- Similar things exist in close proximity\n",
    "- Distance-based algorithm\n",
    "    - So, scaling / standardization is important\n",
    "\n",
    "\n",
    "### Pros\n",
    "\n",
    "- The algorithm is simple and easy to implement.\n",
    "- There is no need to train the model.\n",
    "- There’s only one parameter to tune, and it's fairly straightforward.\n",
    "- The algorithm is versatile. It can be used for classification, regression, and search.\n",
    "- There are no assumptions about the underlying data, so it's well-suited for non-linear data.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Prediction can be computationally expensive and requires high memory storage, especially with a very large dataset.\n",
    "- The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase.\n",
    "- It tends to be less accurate than more sophisticated algorithms.\n",
    "- Sensitive to irrelevant features, subject to the curse of dimensionality. Dimensionality reduction and careful feature selection is recommended.\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "- Pattern recognition\n",
    "- Intrusion / fraud detection\n",
    "- Search and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b299e",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Initialize k to your chosen number of neighbors.\n",
    "2. For each example in the data:\n",
    "    1. Calculate the distance between the query example and the current example from the data.\n",
    "    2. Add the distance and the index of the example to an ordered collection.\n",
    "3. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by distance.\n",
    "4. Pick the first k entries from the sorted collection.\n",
    "5. Get the labels of the selected k entries.\n",
    "6. If regression, return the mean of the k labels. If classification, return the mode of the k labels.\n",
    "\n",
    "### Choosing the right k\n",
    "\n",
    "- Run the KNN algorithm several times with different values of k and choose the k that reduces the number of errors we encounter while maintaining the algorithm’s ability to accurately make predictions when it’s given data it hasn’t seen before.\n",
    "- Things to keep in mind:\n",
    "    - As we decrease the value of k to 1, our predictions become less stable. They can be noisy and the chance of overfitting is higher.\n",
    "    - Inversely, as we increase the value of k, our predictions become more stable due to majority voting / averaging, and thus, more likely to make more accurate predictions (up to a certain point). We can often observe a smoother decision boundary.\n",
    "    - Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of k too far.\n",
    "    - In cases where we are taking a majority vote (e.g. picking the mode in a classification problem) among two labels, we usually make k an odd number to have a tiebreaker.\n",
    "        - In general, it's best practice to set the value of k to not be a multiple of the number of classes present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19370a0d",
   "metadata": {},
   "source": [
    "## Improving the Model\n",
    "\n",
    "- A smart way to select features is to try the model with each feature individually. Pick the best performing one and repeat with the remaining features. Greedily selecting which feature is best based on performance.\n",
    "- Weight votes so the closest neighbors get proportionally more voting power than the further neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16078967",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
