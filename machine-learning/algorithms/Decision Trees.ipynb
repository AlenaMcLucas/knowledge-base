{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d0f6e0",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Summary\n",
    "\n",
    "The goal of using a Decision Tree is to predict the class or value of the target variable by learning simple **decision rules** that split data points. The decision tree algorithm works like a bunch of nested if-else statements wherein successive conditions are checked unless the model reaches a conclusion.\n",
    "\n",
    "\n",
    "**Keywords**:\n",
    "- supervised learning\n",
    "- regression and classification\n",
    "    - more often used in classification\n",
    "- **Root Node**: It represents the entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "- **Splitting**: It is a process of dividing a node into two or more sub-nodes.\n",
    "- **Decision Node**: When a sub-node splits into further sub-nodes, then it is called the decision node.\n",
    "- **Leaf / Terminal Node**: Nodes that do not split are called Leaf or Terminal nodes.\n",
    "- **Pruning**: When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of splitting.\n",
    "- **Branch / Sub-Tree**: A subsection of the entire tree is called branch or sub-tree.\n",
    "- **Parent and Child Node**: A node, which is divided into sub-nodes is called a parent node of sub-nodes, whereas the sub-nodes are called the childen of a parent node.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- In the beginning, the whole training set is considered as the **root**.\n",
    "- Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model.\n",
    "- Records are **distributed recursively** (recursive partitioning, divide and conquer) on the basis of attribute values.\n",
    "- Order to placing attributes as root or internal node of the tree is done by using some statistical approach.\n",
    "- Decision trees apply a **top-down approach** to training.\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Extremely fast prediction\n",
    "- Disregards features that have little to no importance in prediction\n",
    "- Extremely efficient training (provided parameters are reasonable)\n",
    "- Easy to interpret logic (result resembles a flowchart)\n",
    "- Constructs feature importance\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Tends to overfit\n",
    "- Changes in data can lead to unnecessary changes in the result\n",
    "- Large trees can sometimes be difficult to interpret\n",
    "- Biased towards more splits on features with more levels\n",
    "- Can struggle when the target variable has many labels (in classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3767597",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "Each tree has one root node. Generally, the feature with the highest accuracy among all others is chosen as the root node.\n",
    "\n",
    "This is the ID3 (Iterative Dichotomiser) algorithm most taught, and it takes a **greedy search** approach. meaning that it makes the choice that seems to be best at that moment.\n",
    "\n",
    "1. The root node includes the entire dataset.\n",
    "2. Calculate the entropy of the parent (giving us $E(parent)$).\n",
    "3. For each feature in the dataset, split the data by that feature and:\n",
    "    1. Calculate the entropy of each resulting node.\n",
    "    2. Take the weighted average of entropy for each resulting node (giving us $E(parent|new feature)$).\n",
    "    3. Calculate information gain by subtracting $E(parent) - E(parent|new feature)$.\n",
    "4. Repeat steps 2 and 3 for each splitable feature.\n",
    "\n",
    "### Important Parameters\n",
    "\n",
    "- Parameters that control splitting of a decision tree:\n",
    "    - `max_depth`: Maximum depth of nodes. As this increases, time to build the tree also increases.\n",
    "    - `min_samples_split`: Minimum number of samples required to do a split.\n",
    "    - `min_samples_leaf`: Minimum number of samples required to be in the leaf node.\n",
    "    - `max_features`: Maximum features to consider when looking for the best split.\n",
    "    - The above parameters are considered to be pre-pruning methods.\n",
    "- Too shallow, and the model can't predict well. Too deep with too many splits, and it may be prone to overfitting.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "- **Entropy**: The measure of uncertainty of a given dataset, describing the degree of randomness of a particular node. When the margin of difference is low, the model has less confidence in it's prediction and entropy is high.\n",
    "- We aim for lower entropy because it represents higher randomness.\n",
    "- For Binary Classification: $Entropy=-p_{+}logp_{+}-p_{-}logp_{-}$\n",
    "    - Where $p_+$ is the probability of the positive class\n",
    "    - [Article showing the math](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/)\n",
    "- For Regression: $Entropy = \\sum-p_i\\times log_2(p_i)$\n",
    "- The higher the entropy, the higher the impurity of the resulting node.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "- **Information Gain**: Used to measure the reduction of uncertainty in the dataset, given some feature. It's a deciding factor of which attribute should be selected for the next data split.\n",
    "- Interpretation of IG = 0.37: the entropy of the dataset with this split will decrease by 0.37.\n",
    "- IG is a statistical property that measures how well a given attribute separates the training examples according to their target values.\n",
    "- For Classification: $IG = E(Y) - E(Y|X)$, or $IG = E(parent) - E(parent|new feature)$\n",
    "    - Where $E(Y)$ is the entropy of the full dataset\n",
    "    - Where $E(Y|X)$ is the entropy of the dataset given some feature\n",
    "    - [Article showing the math](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/)\n",
    "- For Regression: $IG(D_p,f)=I(D_p)-\\frac{N_{left}}{N}I(D_{left})-\\frac{N_{right}}{N}I(D_{right})$\n",
    "- Favors smaller partitions with distinct values\n",
    "\n",
    "### Gini Index\n",
    "\n",
    "- **Gini Index**: Cost function used to evaluate splits in the dataset. It performs only binary splits.\n",
    "- Higher GI implies higher inequality and higher heterogeneity\n",
    "- Steps are\n",
    "    - Calculate Gini for sub-nodes: $Gini=1-\\sum{p_i}^2$\n",
    "        - Where $p_i$ are the probabilites of each class\n",
    "    - Calculate GI for split using the weighted Gini score of each node of that split\n",
    "- Favors larger, easier to implement partitions\n",
    "\n",
    "### Other Attribution Selection Criteria\n",
    "\n",
    "- Gain Ratio\n",
    "- Reduction in Variance\n",
    "- Chi-Square\n",
    "\n",
    "### Pruning\n",
    "\n",
    "- **Pruning**: Removing branches that have little or no significance in the decision-making process. This can help with overfitting.\n",
    "    - **Pre-pruning** is done while growing the tree to stop it growing.\n",
    "    - **Post-pruning** is done after the tree is built.\n",
    "- Pruning is best done with a validation dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff1a0a",
   "metadata": {},
   "source": [
    "## Improving the Model\n",
    "\n",
    "- Because it overfits in most cases, especially when there are a large number of features, you can improve your decision tree with:\n",
    "    - hyperparameter tuning (parameters that limit the size / depth of a tree)\n",
    "    - pruning\n",
    "    - PCA\n",
    "    - random forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78547f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
