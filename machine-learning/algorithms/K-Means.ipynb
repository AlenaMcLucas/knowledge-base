{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71cad3f",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Keywords**:\n",
    "- unsupervised learning\n",
    "- distance-based\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- Groups or clusters similar points together, revealing underlying patterns\n",
    "- Distance-based algorithm\n",
    "    - So, scaling / standardization is important\n",
    "- The main objective is to minimize the sum of distances between the points and their respective cluster centroid\n",
    "\n",
    "#### Overall Model\n",
    "\n",
    "- All data points in a cluster should be similar to each other\n",
    "- The data points from different cluster should be as different as possible\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Performs well if clusters have spherical shapes.\n",
    "- Simple, a solid algorithm to start with.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Performs poorly for complicated geometric shapes.\n",
    "    - In these cases, tranforming to a higher dimensional representation that makes the data linearly separable works well. One example of this is Spectral Clustering.\n",
    "- It gives more weight to bigger clusters, so if a smaller cluster exists it will become divided + assigned to close, bigger clusters.\n",
    "- Can get stuck in a local minima depending on initialization.\n",
    "- Can perform poorly when features are in high dimensions.\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "- Customer segmentation\n",
    "    - Strategizing around these clusters and our understanding of how these customers may evolve over time.\n",
    "- Document clustering\n",
    "- Image segmentation\n",
    "- Recommendation enginges (to find similar items, i.e. Your Daily Mixes on Spotify)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fd16b",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "Because it is distance-based, don't forget to scale values first.\n",
    "\n",
    "1. Choose k.\n",
    "2. Randomly select the centroid for each cluster from data points.\n",
    "3. Assign all points to the closest cluster centroid.\n",
    "4. Compute the centroids of the newly formed clusters.\n",
    "5. Repeat steps 3 and 4 until:\n",
    "    1. Controids of newly formed cluster do not change, or\n",
    "    2. Points remain in the same cluster, or\n",
    "    3. Maximum number of iterations are reached.\n",
    "\n",
    "### Choosing the Right k\n",
    "\n",
    "- The maximum k is equal to the number of observations in the dataset\n",
    "\n",
    "#### Elbow Method\n",
    "\n",
    "- Plot an **elbow curve**, where the x-axis is the number of clusters k and the y-axis is an evaluation metric\n",
    "    - Intertia and the Dunn Index are commonly used as the evaluation metric\n",
    "    - The cluster(s) k where the evaluation metric's decrease becomes constant represents optimal k value(s)\n",
    "- Consider computation cost\n",
    "\n",
    "#### Silhouette Method\n",
    "\n",
    "- The **silhouette coefficient** measures how similar a data point is within-cluster compared to other clusters\n",
    "- $S(i)=\\frac{b(i)-a(i)}{max(a(i), b(i))}$\n",
    "    - Where $S(i)$ is the silhouette coefficient of the data point i\n",
    "    - Where $a(i)$ is the average distance between i and all the other data points in the cluster to which i belongs\n",
    "    - Where $b(i)$ is the average distance from i to all the other clusters to which i does not belong\n",
    "- Then take the average silhouette for every k and plot, with k on the x-axis and the average silhouette on the y-axis\n",
    "- The value of the coefficient is between $[-1, 1]$, with 1 meaning the data points are very compact within their clsuters and far away from the other clusters. Values near 0 denote overlapping clusters.\n",
    "\n",
    "![](img/ai.png)\n",
    "\n",
    "![](img/bi.png)\n",
    "\n",
    "### K-Means++\n",
    "\n",
    "K-Means++ chooses the initial values of the cluster centriods by:\n",
    "\n",
    "1. The first cluster centroid is chosen uniformly at random from the data points.\n",
    "2. Compute distance of each data point from the chosen centroid(s).\n",
    "3. Then, choose the new cluster center from the data points with the largest distance.\n",
    "4. Repeat steps 2 and 3 until k centroids have been chosen.\n",
    "\n",
    "While it's computationally costly, convergence will often happen faster and it will be less likely to converge at a local minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eae26a",
   "metadata": {},
   "source": [
    "## Evaluting the Model\n",
    "\n",
    "- **Inertia**: sum of distances of all points within a cluser from the centroid of that cluster\n",
    "    - We want intertia to be as low as possible\n",
    "    - These distances from points to centroids within a cluster are called **intra-cluster distances**\n",
    "    - Measure of the model's first goal / assumption\n",
    "- **inter-cluster distance**: distance between the centroids of two different clusters\n",
    "- **Dunn index**: takes into account the distance within clusters (inertia) and distance between cluster\n",
    "    - $Dunn\\text{ }index = \\frac{min(\\text{inter-cluster distance})}{max(\\text{intra-cluster distance})}$\n",
    "    - We want to maximize the Dunn index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a60d0",
   "metadata": {},
   "source": [
    "## Improving the Model\n",
    "\n",
    "- Challenge: size of the clusters can be different\n",
    "- Challenge: densities of the original points can be different\n",
    "- Solution: higher number of clusters\n",
    "- Solution: Use K-Means++ to chose the initial values of the cluster centriods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce1775",
   "metadata": {},
   "source": [
    "### Other\n",
    "\n",
    "- The approach kmeans follows to solve the problem is called Expectation-Maximization. The E-step is assigning the data points to the closest cluster. For each data point, it choses the closest centroid by euclidean distance. The M-step is (re-)computing the centroid of each cluster. Taking the mean of all data points within each new cluster.\n",
    "- Additional topic: bisected k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17a7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
