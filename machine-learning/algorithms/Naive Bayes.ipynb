{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3048e744",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Keywords**:\n",
    "- supervised learning\n",
    "- classification\n",
    "    - binary\n",
    "    - multiclass\n",
    "- **generative model** - captures the joint probability $P(X,Y)$ when specifying the hypothetical random process that generates the data\n",
    "    - Here, we generate the the distribution for each label\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "#### Overall Model\n",
    "\n",
    "- **Independence**: All features are independent of one another\n",
    "    - Because this is typically not true in the real world, this is why the algorithm is \"naive\"\n",
    "- **Equal**: All features equally effect the outcome\n",
    "\n",
    "#### Features\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "- **gaussian / normal distribution**\n",
    "    - Calculate the **likelihood**: ![](../img/naive-bayes-gaussian.png)\n",
    "- **multinomial distribution** - best for discrete counts, i.e. how many times a word appears in that text, and tf-idf\n",
    "    - The distribution is parameterized by a vector of $\\theta$s for each class $y$.\n",
    "    - $\\hat{\\theta_{yi}}=\\frac{N_{yi}+\\alpha}{N_y+\\alpha n}$\n",
    "        - where $n$ is the number of features (in text classification, the size of the vocabulary)\n",
    "        - where $\\theta_{yi}$ is the probability $P(x_i|y)$ of feature $i$ appearing in a sample belonging to class $y$\n",
    "        - where $N_{yi}$ iis the number of times featire $i$ appears in a sample of class $y$ in the training set\n",
    "        - where $N_y$ is the total count of all feature sfor class $y$\n",
    "        - setting $\\alpha=1$ is Laplace smoothing, and $\\alpha < 1$ is Lidstone smoothing.\n",
    "    - This is a smoothed verion of maxikum likelihood that involves relative frequency counting\n",
    "- **bernoulli distribution** - best if your feature vectors are binary, like in text classification\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Fast prediction, even for very large and/or highly-dimensional datasets\n",
    "- Relatively simple and interpretable\n",
    "- Requires a small amount of training data to estimate the necessary parameters\n",
    "- Few (if any) parameters to tune\n",
    "- Can outperform sophisticated classification methods, especially when the independence assumption holds\n",
    "- Categorical input variables perform well\n",
    "- Predicted posterior probabilities can provide an estimate of uncertainty\n",
    "\n",
    "### Cons\n",
    "\n",
    "- If a categorical variable was not observered in training, the model assigns a 0 probability to it and will be unable to make a prediction.\n",
    "    - To avoid this, we use smoothing techniques.\n",
    "- Assumptions, especially the independence assumption, are often violated in practice. So, it's naive and known as a bad estimator. Therefore, the predicted probability `predict_proba` should not be taken too seriously.\n",
    "- If there are many numerical variables, a normal distribution is assumed (strong assumption)\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "- Text classification / spam filtering / sentiment analysis\n",
    "- Multiclass classification\n",
    "- Real-time predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf3bde",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Bayes theorem provides a way of calculating posterior probability $P(c|x)$ from $P(c)$, $P(x)$ and $P(x|c)$:\n",
    "\n",
    "![](../img/bayes-theorem.png)\n",
    "\n",
    "- $x$ is the **evidence**, since it's already happened.\n",
    "- $c$ is the **hypothesis** / class, since we are estimating its probability given the evidence.\n",
    "- $P(c|x)$ is the **posterior probability** of class (c, target) given predictor (x, attributes). It's the probability of an event after the evidence is observed.\n",
    "- $P(c)$ is the **priori**, or **prior probability** of the class. It's the probability of the event before the evidence was observed.\n",
    "- $P(x|c)$ is the **likelihood** which is the probability of predictor given class.\n",
    "- $P(x)$ is the **prior probability** of predictor.\n",
    "\n",
    "### Applied to Multiple Features\n",
    "\n",
    "By substituting for X and expanding using the chain rule we get:\n",
    "\n",
    "![](../img/bayes-theorem-multiple-features.png)\n",
    "\n",
    "For all entries in the dataset, the denominator does not change, it remain static.\n",
    "\n",
    "### Applied to a Classifier\n",
    "\n",
    "- Calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.\n",
    "\n",
    "### Smoothing Techniques\n",
    "\n",
    "- **zero-frequency problem** - if you have no occurrences of a class label and a certain attribute value together, then the frequency-based probability estimate will be zero. And this will get a zero when all the probabilities are multiplied. To address this, we can use smoothing.\n",
    "- **Laplace smoothing**: +1 numerator, +2 denominator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e04b99",
   "metadata": {},
   "source": [
    "## Improving the Model\n",
    "\n",
    "- If continuous features do not. have a normal distribution, we should use transformation or different methods to convert it in normal distribution.\n",
    "- If the test data set has the zero frequency issue (data unobserved in training), apply smoothing techniques to predict the classes of the test data set.\n",
    "- Remove correlated features, as the highly correlated features are voted twice in the model and it can lead to over-inflating importance.\n",
    "- There are not many (if any) parameters to tune. It's better to focus on feature selection and engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2862e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
